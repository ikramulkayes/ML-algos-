{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "914f6477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f62429cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bddf567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',        #as we gotta scale the images test set and traning set defferently as we want to keep them apart to avoid information lickage\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f45562f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential() #starting this as we gotta make the images into vectors and first two steps here are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b8e39e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3])) #as we gotta make the input shape 64 as the image size is converged to 64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c60f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d3efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu')) # as for second convolutional layer we dont need to set the input parameter\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "641a4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten()) #it is the thing which converts into vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eab64aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=64, activation='relu')) #it is the input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b92e8df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=64, activation='relu')) #it is second the input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0b7fb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))#output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7518dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) #preparing to train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec379606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 0.6617 - accuracy: 0.5931 - val_loss: 0.6191 - val_accuracy: 0.6910\n",
      "Epoch 2/30\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 0.5937 - accuracy: 0.6866 - val_loss: 0.5457 - val_accuracy: 0.7330\n",
      "Epoch 3/30\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 0.5614 - accuracy: 0.7020 - val_loss: 0.5139 - val_accuracy: 0.7580\n",
      "Epoch 4/30\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.5326 - accuracy: 0.7276 - val_loss: 0.5035 - val_accuracy: 0.7585\n",
      "Epoch 5/30\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.5016 - accuracy: 0.7588 - val_loss: 0.4942 - val_accuracy: 0.7640\n",
      "Epoch 6/30\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.4838 - accuracy: 0.7636 - val_loss: 0.4943 - val_accuracy: 0.7580\n",
      "Epoch 7/30\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.4755 - accuracy: 0.7720 - val_loss: 0.4691 - val_accuracy: 0.7850\n",
      "Epoch 8/30\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.4456 - accuracy: 0.7860 - val_loss: 0.4578 - val_accuracy: 0.7875\n",
      "Epoch 9/30\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.4429 - accuracy: 0.7910 - val_loss: 0.4790 - val_accuracy: 0.7725\n",
      "Epoch 10/30\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 0.4237 - accuracy: 0.8025 - val_loss: 0.4662 - val_accuracy: 0.7990\n",
      "Epoch 11/30\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 0.4063 - accuracy: 0.8158 - val_loss: 0.5348 - val_accuracy: 0.7550\n",
      "Epoch 12/30\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 0.4041 - accuracy: 0.8124 - val_loss: 0.4624 - val_accuracy: 0.7970\n",
      "Epoch 13/30\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.3884 - accuracy: 0.8235 - val_loss: 0.4684 - val_accuracy: 0.7940\n",
      "Epoch 14/30\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 0.3755 - accuracy: 0.8255 - val_loss: 0.4927 - val_accuracy: 0.7835\n",
      "Epoch 15/30\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.3591 - accuracy: 0.8397 - val_loss: 0.4611 - val_accuracy: 0.8060\n",
      "Epoch 16/30\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.3571 - accuracy: 0.8401 - val_loss: 0.4895 - val_accuracy: 0.7900\n",
      "Epoch 17/30\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.3401 - accuracy: 0.8509 - val_loss: 0.4760 - val_accuracy: 0.8005\n",
      "Epoch 18/30\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.3343 - accuracy: 0.8469 - val_loss: 0.4962 - val_accuracy: 0.7895\n",
      "Epoch 19/30\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 0.3332 - accuracy: 0.8550 - val_loss: 0.6102 - val_accuracy: 0.7605\n",
      "Epoch 20/30\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.3109 - accuracy: 0.8666 - val_loss: 0.4880 - val_accuracy: 0.7975\n",
      "Epoch 21/30\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.3026 - accuracy: 0.8687 - val_loss: 0.5202 - val_accuracy: 0.7915\n",
      "Epoch 22/30\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 0.2884 - accuracy: 0.8729 - val_loss: 0.5281 - val_accuracy: 0.7880\n",
      "Epoch 23/30\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 0.2913 - accuracy: 0.8748 - val_loss: 0.5105 - val_accuracy: 0.7945\n",
      "Epoch 24/30\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 0.2751 - accuracy: 0.8806 - val_loss: 0.5712 - val_accuracy: 0.7845\n",
      "Epoch 25/30\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 0.2753 - accuracy: 0.8821 - val_loss: 0.5062 - val_accuracy: 0.7900\n",
      "Epoch 26/30\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.2631 - accuracy: 0.8906 - val_loss: 0.5350 - val_accuracy: 0.8000\n",
      "Epoch 27/30\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.2682 - accuracy: 0.8886 - val_loss: 0.5515 - val_accuracy: 0.7860\n",
      "Epoch 28/30\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 0.2499 - accuracy: 0.8917 - val_loss: 0.5292 - val_accuracy: 0.7985\n",
      "Epoch 29/30\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 0.2475 - accuracy: 0.8971 - val_loss: 0.5381 - val_accuracy: 0.8115\n",
      "Epoch 30/30\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.2348 - accuracy: 0.9044 - val_loss: 0.5346 - val_accuracy: 0.8090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25c17321c70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install --upgrade tensorflow keras numpy pandas sklearn pillow\n",
    "import sys\n",
    "import PIL\n",
    "#sys.modules['Image'] = Image \n",
    "\n",
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 30) #as we need te validate the traning set by comparing with the test set result but it is not using test set to train but it is using it to validate the output and reshaping its algorithm and nurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "969689b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/baby2.png', target_size = (64, 64)) #importing test image\n",
    "test_image = image.img_to_array(test_image) # converting the image into numpy array\n",
    "test_image = np.expand_dims(test_image, axis = 0) #as the axis or dimension of that aarray is 1\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e40cb91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc6c151",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
